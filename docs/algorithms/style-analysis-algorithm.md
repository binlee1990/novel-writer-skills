# æ–‡æœ¬åˆ†æç®—æ³•å®ç°æŒ‡å—

æœ¬æ–‡æ¡£è¯¦ç»†è¯´æ˜å¦‚ä½•å®ç°è‡ªé€‚åº”é£æ ¼å­¦ä¹ çš„æ–‡æœ¬åˆ†æç®—æ³•ã€‚

---

## æ¦‚è¿°

é£æ ¼åˆ†æç®—æ³•çš„ç›®æ ‡æ˜¯ä»ç”¨æˆ·å·²å†™çš„ç« èŠ‚ä¸­æå–å¯é‡åŒ–çš„é£æ ¼ç‰¹å¾ï¼Œå¹¶ç”Ÿæˆä¸ªæ€§åŒ–çš„é£æ ¼æŒ‡å—ã€‚

**æ ¸å¿ƒæ€è·¯**ï¼š
1. è¯»å–ç”¨æˆ·å·²å†™ç« èŠ‚ï¼ˆ3+ ç« ï¼‰
2. å¯¹æ–‡æœ¬è¿›è¡Œå¤šå±‚æ¬¡åˆ†æï¼ˆå¥å­ã€è¯æ±‡ã€æ®µè½ã€å¯¹è¯ã€èŠ‚å¥ï¼‰
3. æå–é‡åŒ–æŒ‡æ ‡
4. ç”Ÿæˆå®šæ€§æè¿°
5. è¾“å‡ºå…·ä½“çš„ç»­å†™æŒ‡å¯¼

---

## ç®—æ³•æ¶æ„

```
è¾“å…¥ï¼šç”¨æˆ·å·²å†™ç« èŠ‚ï¼ˆMarkdown æ–‡ä»¶ï¼‰
  â†“
é¢„å¤„ç†ï¼šæ¸…ç†æ ¼å¼ã€åˆ†å‰²æ–‡æœ¬
  â†“
å¤šå±‚æ¬¡åˆ†æï¼š
  â”œâ”€ å¥å­åˆ†æ
  â”œâ”€ è¯æ±‡åˆ†æ
  â”œâ”€ æ®µè½åˆ†æ
  â”œâ”€ å¯¹è¯åˆ†æ
  â””â”€ èŠ‚å¥åˆ†æ
  â†“
ç‰¹å¾æå–ï¼šé‡åŒ–æŒ‡æ ‡
  â†“
é£æ ¼å½’çº³ï¼šå®šæ€§æè¿°
  â†“
è¾“å‡ºï¼špersonal-voice.md
```

---

## 1. é¢„å¤„ç†é˜¶æ®µ

### 1.1 è¯»å–ç« èŠ‚æ–‡ä»¶

**ç›®æ ‡**ï¼šè¯»å–ç”¨æˆ·å·²å†™çš„ç« èŠ‚å†…å®¹

**å®ç°æ­¥éª¤**ï¼š
1. æ‰«æ `stories/[current]/content/` ç›®å½•
2. è¯†åˆ«ç« èŠ‚æ–‡ä»¶ï¼ˆé€šå¸¸æ˜¯ `chapter-*.md` æˆ– `ç¬¬*ç« .md`ï¼‰
3. æ’é™¤å¤§çº²æ–‡ä»¶ã€è‰ç¨¿æ–‡ä»¶
4. è¯»å–æœ€è¿‘ 10 ç« ï¼ˆæˆ–å…¨éƒ¨ï¼Œå¦‚æœ < 10 ç« ï¼‰

**ä¼ªä»£ç **ï¼š
```python
def read_chapters(story_path: str, max_chapters: int = 10) -> List[str]:
    """è¯»å–ç« èŠ‚å†…å®¹"""
    content_dir = os.path.join(story_path, "content")

    # æŸ¥æ‰¾ç« èŠ‚æ–‡ä»¶
    chapter_files = []
    for file in os.listdir(content_dir):
        if file.startswith("chapter-") or file.startswith("ç¬¬"):
            if file.endswith(".md"):
                chapter_files.append(file)

    # æŒ‰ç« èŠ‚å·æ’åº
    chapter_files.sort()

    # è¯»å–æœ€è¿‘çš„ç« èŠ‚
    recent_chapters = chapter_files[-max_chapters:]

    chapters = []
    for file in recent_chapters:
        with open(os.path.join(content_dir, file), 'r', encoding='utf-8') as f:
            content = f.read()
            chapters.append(content)

    return chapters
```

### 1.2 æ¸…ç†æ–‡æœ¬

**ç›®æ ‡**ï¼šç§»é™¤ Markdown æ ¼å¼æ ‡è®°ï¼Œä¿ç•™çº¯æ–‡æœ¬

**éœ€è¦æ¸…ç†çš„å†…å®¹**ï¼š
- æ ‡é¢˜æ ‡è®°ï¼ˆ`#`ï¼‰
- ç²—ä½“/æ–œä½“æ ‡è®°ï¼ˆ`**`ã€`*`ï¼‰
- é“¾æ¥æ ‡è®°ï¼ˆ`[]()`ï¼‰
- ä»£ç å—æ ‡è®°ï¼ˆ` ``` `ï¼‰
- æ³¨é‡Šï¼ˆ`<!-- -->`ï¼‰

**ä¿ç•™çš„å†…å®¹**ï¼š
- æ®µè½åˆ†éš”ï¼ˆç©ºè¡Œï¼‰
- å¯¹è¯å¼•å·ï¼ˆ`""`ã€`ã€Œã€`ã€`ã€ã€`ï¼‰
- æ ‡ç‚¹ç¬¦å·

**ä¼ªä»£ç **ï¼š
```python
import re

def clean_markdown(text: str) -> str:
    """æ¸…ç† Markdown æ ¼å¼"""
    # ç§»é™¤æ ‡é¢˜æ ‡è®°
    text = re.sub(r'^#+\s+', '', text, flags=re.MULTILINE)

    # ç§»é™¤ç²—ä½“/æ–œä½“
    text = re.sub(r'\*\*(.+?)\*\*', r'\1', text)
    text = re.sub(r'\*(.+?)\*', r'\1', text)

    # ç§»é™¤é“¾æ¥
    text = re.sub(r'\[(.+?)\]\(.+?\)', r'\1', text)

    # ç§»é™¤ä»£ç å—
    text = re.sub(r'```[\s\S]*?```', '', text)

    # ç§»é™¤æ³¨é‡Š
    text = re.sub(r'<!--[\s\S]*?-->', '', text)

    return text.strip()
```

---

## 2. å¥å­å±‚é¢åˆ†æ

### 2.1 å¥å­åˆ†å‰²

**ç›®æ ‡**ï¼šå°†æ–‡æœ¬åˆ†å‰²æˆç‹¬ç«‹çš„å¥å­

**ä¸­æ–‡å¥å­åˆ†å‰²è§„åˆ™**ï¼š
- å¥æœ«æ ‡ç‚¹ï¼š`ã€‚`ã€`ï¼`ã€`ï¼Ÿ`ã€`ï¼›`
- ç‰¹æ®Šå¤„ç†ï¼šå¼•å·å†…çš„æ ‡ç‚¹ã€çœç•¥å·ã€ç ´æŠ˜å·

**å®ç°æ­¥éª¤**ï¼š
1. æŒ‰å¥æœ«æ ‡ç‚¹åˆ†å‰²
2. å¤„ç†å¼•å·å†…çš„å¥å­
3. å¤„ç†çœç•¥å·ï¼ˆ`â€¦â€¦`ï¼‰å’Œç ´æŠ˜å·ï¼ˆ`â€”â€”`ï¼‰

**ä¼ªä»£ç **ï¼š
```python
def split_sentences(text: str) -> List[str]:
    """åˆ†å‰²å¥å­"""
    # å¥æœ«æ ‡ç‚¹
    sentence_endings = ['ã€‚', 'ï¼', 'ï¼Ÿ', 'ï¼›']

    sentences = []
    current_sentence = ""
    in_quote = False

    for i, char in enumerate(text):
        current_sentence += char

        # æ£€æµ‹å¼•å·çŠ¶æ€
        if char in ['"', 'ã€Œ', 'ã€']:
            in_quote = True
        elif char in ['"', 'ã€', 'ã€']:
            in_quote = False

        # å¥æœ«æ ‡ç‚¹ä¸”ä¸åœ¨å¼•å·å†…
        if char in sentence_endings and not in_quote:
            sentences.append(current_sentence.strip())
            current_sentence = ""

    # æ·»åŠ æœ€åä¸€å¥
    if current_sentence.strip():
        sentences.append(current_sentence.strip())

    return sentences
```

### 2.2 è®¡ç®—å¥å­é•¿åº¦

**ç›®æ ‡**ï¼šç»Ÿè®¡æ¯ä¸ªå¥å­çš„å­—æ•°

**å®ç°æ­¥éª¤**ï¼š
1. å¯¹æ¯ä¸ªå¥å­è®¡ç®—å­—ç¬¦æ•°ï¼ˆä¸å«æ ‡ç‚¹ï¼‰
2. è®¡ç®—å¹³å‡å¥é•¿
3. è®¡ç®—å¥é•¿æ ‡å‡†å·®
4. ç»Ÿè®¡çŸ­å¥ã€ä¸­å¥ã€é•¿å¥æ¯”ä¾‹

**ä¼ªä»£ç **ï¼š
```python
import re
from typing import Dict

def analyze_sentence_length(sentences: List[str]) -> Dict:
    """åˆ†æå¥å­é•¿åº¦"""
    # ç§»é™¤æ ‡ç‚¹ï¼Œè®¡ç®—å­—æ•°
    lengths = []
    for sentence in sentences:
        # ç§»é™¤æ‰€æœ‰æ ‡ç‚¹
        clean_sentence = re.sub(r'[ï¼Œã€‚ï¼ï¼Ÿï¼›ï¼š""''ã€Œã€ã€ã€ã€]', '', sentence)
        length = len(clean_sentence)
        if length > 0:
            lengths.append(length)

    if not lengths:
        return {}

    # è®¡ç®—ç»Ÿè®¡æŒ‡æ ‡
    avg_length = sum(lengths) / len(lengths)
    std_dev = (sum((x - avg_length) ** 2 for x in lengths) / len(lengths)) ** 0.5

    # åˆ†ç±»ç»Ÿè®¡
    short_sentences = sum(1 for l in lengths if l < 15)
    medium_sentences = sum(1 for l in lengths if 15 <= l <= 30)
    long_sentences = sum(1 for l in lengths if l > 30)

    total = len(lengths)

    return {
        'avg_length': round(avg_length, 1),
        'std_dev': round(std_dev, 1),
        'short_ratio': round(short_sentences / total * 100, 1),
        'medium_ratio': round(medium_sentences / total * 100, 1),
        'long_ratio': round(long_sentences / total * 100, 1),
        'total_sentences': total
    }
```

---

## 3. è¯æ±‡å±‚é¢åˆ†æ

### 3.1 åˆ†è¯å’Œè¯æ€§æ ‡æ³¨

**ç›®æ ‡**ï¼šè¯†åˆ«è¯æ±‡å¹¶æ ‡æ³¨è¯æ€§

**æ¨èå·¥å…·**ï¼šjieba åˆ†è¯åº“

**å®ç°æ­¥éª¤**ï¼š
1. ä½¿ç”¨ jieba è¿›è¡Œåˆ†è¯
2. ä½¿ç”¨ jieba.posseg è¿›è¡Œè¯æ€§æ ‡æ³¨
3. è¯†åˆ«å½¢å®¹è¯ã€å‰¯è¯ã€åŠ¨è¯ç­‰

**ä¼ªä»£ç **ï¼š
```python
import jieba.posseg as pseg

def tokenize_and_tag(text: str) -> List[Tuple[str, str]]:
    """åˆ†è¯å’Œè¯æ€§æ ‡æ³¨"""
    words = pseg.cut(text)
    return [(word, flag) for word, flag in words]
```

### 3.2 å½¢å®¹è¯å’Œå‰¯è¯å¯†åº¦

**ç›®æ ‡**ï¼šè®¡ç®—å½¢å®¹è¯å’Œå‰¯è¯å æ€»è¯æ•°çš„æ¯”ä¾‹

**è¯æ€§æ ‡è®°**ï¼š
- å½¢å®¹è¯ï¼š`a`, `ad`, `an`, `ag`
- å‰¯è¯ï¼š`d`, `df`, `dg`

**ä¼ªä»£ç **ï¼š
```python
def analyze_word_density(text: str) -> Dict:
    """åˆ†æè¯æ±‡å¯†åº¦"""
    words = tokenize_and_tag(text)

    total_words = len(words)
    adjectives = sum(1 for _, flag in words if flag.startswith('a'))
    adverbs = sum(1 for _, flag in words if flag.startswith('d'))

    return {
        'adj_density': round(adjectives / total_words * 100, 1),
        'adv_density': round(adverbs / total_words * 100, 1),
        'total_words': total_words
    }
```

### 3.3 é«˜é¢‘è¯ç»Ÿè®¡

**ç›®æ ‡**ï¼šæ‰¾å‡ºæœ€å¸¸ç”¨çš„è¯æ±‡

**å®ç°æ­¥éª¤**ï¼š
1. ç»Ÿè®¡æ¯ä¸ªè¯çš„å‡ºç°æ¬¡æ•°
2. æ’é™¤åœç”¨è¯ï¼ˆçš„ã€äº†ã€æ˜¯ã€åœ¨ç­‰ï¼‰
3. è¿”å› Top 10 é«˜é¢‘è¯

**ä¼ªä»£ç **ï¼š
```python
from collections import Counter

def get_frequent_words(text: str, top_n: int = 10) -> List[Tuple[str, int]]:
    """è·å–é«˜é¢‘è¯"""
    words = tokenize_and_tag(text)

    # åœç”¨è¯åˆ—è¡¨
    stopwords = {'çš„', 'äº†', 'æ˜¯', 'åœ¨', 'æˆ‘', 'æœ‰', 'å’Œ', 'å°±', 'ä¸', 'äºº', 'éƒ½', 'ä¸€', 'ä¸€ä¸ª', 'ä¸Š', 'ä¹Ÿ', 'å¾ˆ', 'åˆ°', 'è¯´', 'è¦', 'å»', 'ä½ ', 'ä¼š', 'ç€', 'æ²¡æœ‰', 'çœ‹', 'å¥½', 'è‡ªå·±', 'è¿™'}

    # è¿‡æ»¤åœç”¨è¯å’Œæ ‡ç‚¹
    filtered_words = [word for word, flag in words
                     if word not in stopwords and flag != 'x']

    # ç»Ÿè®¡é¢‘ç‡
    word_counts = Counter(filtered_words)

    return word_counts.most_common(top_n)
```

---

## 4. æ®µè½å±‚é¢åˆ†æ

### 4.1 æ®µè½åˆ†å‰²

**ç›®æ ‡**ï¼šå°†æ–‡æœ¬åˆ†å‰²æˆæ®µè½

**åˆ†å‰²è§„åˆ™**ï¼š
- ä»¥ç©ºè¡Œï¼ˆ`\n\n`ï¼‰ä¸ºæ®µè½åˆ†éš”ç¬¦
- å•è¡Œä¹Ÿç®—ä¸€ä¸ªæ®µè½

**ä¼ªä»£ç **ï¼š
```python
def split_paragraphs(text: str) -> List[str]:
    """åˆ†å‰²æ®µè½"""
    # æŒ‰ç©ºè¡Œåˆ†å‰²
    paragraphs = text.split('\n\n')

    # æ¸…ç†ç©ºæ®µè½
    paragraphs = [p.strip() for p in paragraphs if p.strip()]

    return paragraphs
```

### 4.2 æ®µè½é•¿åº¦åˆ†æ

**ç›®æ ‡**ï¼šç»Ÿè®¡æ®µè½é•¿åº¦å’Œå•å¥æ®µæ¯”ä¾‹

**å®ç°æ­¥éª¤**ï¼š
1. è®¡ç®—æ¯ä¸ªæ®µè½çš„å­—æ•°
2. è®¡ç®—å¹³å‡æ®µé•¿
3. ç»Ÿè®¡å•å¥æ®µï¼ˆåªæœ‰ä¸€ä¸ªå¥å­çš„æ®µè½ï¼‰æ¯”ä¾‹

**ä¼ªä»£ç **ï¼š
```python
def analyze_paragraph_length(paragraphs: List[str]) -> Dict:
    """åˆ†ææ®µè½é•¿åº¦"""
    lengths = []
    single_sentence_paras = 0

    for para in paragraphs:
        # è®¡ç®—å­—æ•°
        clean_para = re.sub(r'[ï¼Œã€‚ï¼ï¼Ÿï¼›ï¼š""''ã€Œã€ã€ã€ã€]', '', para)
        length = len(clean_para)
        lengths.append(length)

        # æ£€æµ‹æ˜¯å¦ä¸ºå•å¥æ®µ
        sentences = split_sentences(para)
        if len(sentences) == 1:
            single_sentence_paras += 1

    avg_length = sum(lengths) / len(lengths) if lengths else 0
    single_para_ratio = single_sentence_paras / len(paragraphs) * 100 if paragraphs else 0

    return {
        'avg_length': round(avg_length, 1),
        'single_para_ratio': round(single_para_ratio, 1),
        'total_paragraphs': len(paragraphs)
    }
```

### 4.3 æ®µè½ç±»å‹åˆ†ç±»

**ç›®æ ‡**ï¼šè¯†åˆ«å¯¹è¯æ®µã€æå†™æ®µã€å™è¿°æ®µ

**åˆ†ç±»è§„åˆ™**ï¼š
- **å¯¹è¯æ®µ**ï¼šåŒ…å«å¼•å· `""`ã€`ã€Œã€`ã€`ã€ã€`ï¼Œä¸”å¼•å·å†…å®¹å æ¯” > 50%
- **æå†™æ®µ**ï¼šå½¢å®¹è¯å¯†åº¦ > 10%
- **å™è¿°æ®µ**ï¼šå…¶ä»–

**ä¼ªä»£ç **ï¼š
```python
def classify_paragraph_type(para: str) -> str:
    """åˆ†ç±»æ®µè½ç±»å‹"""
    # æ£€æµ‹å¯¹è¯
    dialogue_chars = sum(1 for char in para if char in '"ã€Œã€')
    if dialogue_chars > 0:
        # è®¡ç®—å¼•å·å†…å®¹å æ¯”
        dialogue_content = re.findall(r'[""ã€Œã€](.+?)[""ã€ã€]', para)
        dialogue_length = sum(len(content) for content in dialogue_content)
        if dialogue_length / len(para) > 0.5:
            return 'dialogue'

    # æ£€æµ‹æå†™ï¼ˆå½¢å®¹è¯å¯†åº¦ï¼‰
    words = tokenize_and_tag(para)
    adjectives = sum(1 for _, flag in words if flag.startswith('a'))
    if adjectives / len(words) > 0.1:
        return 'description'

    # é»˜è®¤ä¸ºå™è¿°
    return 'narration'

def analyze_paragraph_types(paragraphs: List[str]) -> Dict:
    """åˆ†ææ®µè½ç±»å‹åˆ†å¸ƒ"""
    types = [classify_paragraph_type(para) for para in paragraphs]

    dialogue_count = types.count('dialogue')
    description_count = types.count('description')
    narration_count = types.count('narration')

    total = len(types)

    return {
        'dialogue_ratio': round(dialogue_count / total * 100, 1),
        'description_ratio': round(description_count / total * 100, 1),
        'narration_ratio': round(narration_count / total * 100, 1)
    }
```

---

## 5. å¯¹è¯å±‚é¢åˆ†æ

### 5.1 å¯¹è¯æ£€æµ‹

**ç›®æ ‡**ï¼šè¯†åˆ«å¯¹è¯å†…å®¹å¹¶è®¡ç®—å¯¹è¯å æ¯”

**æ£€æµ‹è§„åˆ™**ï¼š
- è¯†åˆ«å¼•å·ï¼š`""`ã€`ã€Œã€`ã€`ã€ã€`
- æå–å¼•å·å†…çš„å†…å®¹
- è®¡ç®—å¯¹è¯å­—æ•°å æ€»å­—æ•°çš„æ¯”ä¾‹

**ä¼ªä»£ç **ï¼š
```python
def analyze_dialogue(text: str) -> Dict:
    """åˆ†æå¯¹è¯å æ¯”"""
    # æå–å¯¹è¯å†…å®¹
    dialogue_pattern = r'[""ã€Œã€](.+?)[""ã€ã€]'
    dialogues = re.findall(dialogue_pattern, text)

    # è®¡ç®—å¯¹è¯å­—æ•°
    dialogue_chars = sum(len(d) for d in dialogues)

    # è®¡ç®—æ€»å­—æ•°ï¼ˆä¸å«æ ‡ç‚¹ï¼‰
    clean_text = re.sub(r'[ï¼Œã€‚ï¼ï¼Ÿï¼›ï¼š""''ã€Œã€ã€ã€ã€\s]', '', text)
    total_chars = len(clean_text)

    dialogue_ratio = dialogue_chars / total_chars * 100 if total_chars > 0 else 0

    return {
        'dialogue_ratio': round(dialogue_ratio, 1),
        'dialogue_count': len(dialogues)
    }
```

### 5.2 å¯¹è¯æ ‡ç­¾é¢‘ç‡

**ç›®æ ‡**ï¼šç»Ÿè®¡å¯¹è¯æ ‡ç­¾ï¼ˆ"ä»–è¯´"ã€"å¥¹è¯´"ï¼‰çš„ä½¿ç”¨é¢‘ç‡

**æ£€æµ‹è§„åˆ™**ï¼š
- è¯†åˆ«å¸¸è§å¯¹è¯æ ‡ç­¾ï¼šè¯´ã€é—®ã€ç­”ã€é“ã€å–Šã€å«ç­‰
- è®¡ç®—å¯¹è¯æ ‡ç­¾å å¯¹è¯æ€»æ•°çš„æ¯”ä¾‹

**ä¼ªä»£ç **ï¼š
```python
def analyze_dialogue_tags(text: str) -> Dict:
    """åˆ†æå¯¹è¯æ ‡ç­¾é¢‘ç‡"""
    # æå–å¯¹è¯
    dialogues = re.findall(r'[""ã€Œã€](.+?)[""ã€ã€]', text)

    # æ£€æµ‹å¯¹è¯æ ‡ç­¾
    tag_pattern = r'[""ã€ã€](ä»–|å¥¹|æˆ‘|ä½ |[^ï¼Œã€‚ï¼ï¼Ÿï¼›ï¼š]{1,3})(è¯´|é—®|ç­”|é“|å–Š|å«|ç¬‘|å“­)'
    tags = re.findall(tag_pattern, text)

    tag_ratio = len(tags) / len(dialogues) * 100 if dialogues else 0

    return {
        'tag_frequency': round(tag_ratio, 1),
        'tag_count': len(tags)
    }
```

---

## 6. èŠ‚å¥å±‚é¢åˆ†æ

### 6.1 åœºæ™¯åˆ‡æ¢æ£€æµ‹

**ç›®æ ‡**ï¼šè¯†åˆ«åœºæ™¯åˆ‡æ¢å¹¶è®¡ç®—åœºæ™¯é¢‘ç‡

**æ£€æµ‹è§„åˆ™**ï¼š
- ç©ºè¡Œï¼ˆæ®µè½åˆ†éš”ï¼‰
- æ—¶é—´æ ‡è®°ï¼ˆ"ç¬¬äºŒå¤©"ã€"ä¸‰å¤©å"ï¼‰
- åœ°ç‚¹æ ‡è®°ï¼ˆ"åœ¨å’–å•¡é¦†"ã€"å›åˆ°å®¶"ï¼‰

**ä¼ªä»£ç **ï¼š
```python
def detect_scene_changes(text: str) -> int:
    """æ£€æµ‹åœºæ™¯åˆ‡æ¢æ¬¡æ•°"""
    scene_changes = 0

    # æ£€æµ‹ç©ºè¡Œï¼ˆæ®µè½åˆ†éš”ï¼‰
    paragraphs = split_paragraphs(text)
    scene_changes += len(paragraphs) - 1

    # æ£€æµ‹æ—¶é—´æ ‡è®°
    time_markers = ['ç¬¬äºŒå¤©', 'ä¸‰å¤©å', 'ä¸€å‘¨å', 'æ¬¡æ—¥', 'ç¿Œæ—¥', 'è¿‡äº†', 'åæ¥']
    for marker in time_markers:
        scene_changes += text.count(marker)

    # æ£€æµ‹åœ°ç‚¹æ ‡è®°
    location_markers = ['åœ¨', 'åˆ°äº†', 'æ¥åˆ°', 'å›åˆ°', 'èµ°è¿›']
    for marker in location_markers:
        scene_changes += text.count(marker)

    return scene_changes

def analyze_scene_frequency(chapters: List[str]) -> Dict:
    """åˆ†æåœºæ™¯åˆ‡æ¢é¢‘ç‡"""
    total_scenes = sum(detect_scene_changes(chapter) for chapter in chapters)
    avg_scenes_per_chapter = total_scenes / len(chapters) if chapters else 0

    return {
        'avg_scenes_per_chapter': round(avg_scenes_per_chapter, 1),
        'total_scenes': total_scenes
    }
```

---

## 7. ç»¼åˆåˆ†æå’Œé£æ ¼å½’çº³

### 7.1 æ•´åˆæ‰€æœ‰æŒ‡æ ‡

**ç›®æ ‡**ï¼šå°†æ‰€æœ‰åˆ†æç»“æœæ•´åˆæˆä¸€ä¸ªé£æ ¼æ¡£æ¡ˆ

**ä¼ªä»£ç **ï¼š
```python
def analyze_writing_style(chapters: List[str]) -> Dict:
    """ç»¼åˆåˆ†æå†™ä½œé£æ ¼"""
    # åˆå¹¶æ‰€æœ‰ç« èŠ‚
    full_text = '\n\n'.join(chapters)

    # æ¸…ç†æ–‡æœ¬
    clean_text = clean_markdown(full_text)

    # åˆ†å‰²æ–‡æœ¬
    sentences = split_sentences(clean_text)
    paragraphs = split_paragraphs(clean_text)

    # æ‰§è¡Œå„å±‚æ¬¡åˆ†æ
    sentence_analysis = analyze_sentence_length(sentences)
    word_analysis = analyze_word_density(clean_text)
    frequent_words = get_frequent_words(clean_text)
    paragraph_analysis = analyze_paragraph_length(paragraphs)
    paragraph_types = analyze_paragraph_types(paragraphs)
    dialogue_analysis = analyze_dialogue(clean_text)
    dialogue_tags = analyze_dialogue_tags(clean_text)
    scene_analysis = analyze_scene_frequency(chapters)

    # æ•´åˆç»“æœ
    style_profile = {
        'sentence': sentence_analysis,
        'word': word_analysis,
        'frequent_words': frequent_words,
        'paragraph': paragraph_analysis,
        'paragraph_types': paragraph_types,
        'dialogue': dialogue_analysis,
        'dialogue_tags': dialogue_tags,
        'scene': scene_analysis
    }

    return style_profile
```

### 7.2 ç”Ÿæˆå®šæ€§æè¿°

**ç›®æ ‡**ï¼šæ ¹æ®é‡åŒ–æŒ‡æ ‡ç”Ÿæˆå®šæ€§çš„é£æ ¼æè¿°

**è§„åˆ™**ï¼š
- å¹³å‡å¥é•¿ < 18 å­— â†’ "çŸ­å¥ä¸ºä¸»"
- å¹³å‡å¥é•¿ 18-25 å­— â†’ "ä¸­çŸ­å¥æ··åˆ"
- å¹³å‡å¥é•¿ > 25 å­— â†’ "é•¿å¥ä¸ºä¸»"
- å¯¹è¯å æ¯” > 60% â†’ "å¯¹è¯é©±åŠ¨"
- å¯¹è¯å æ¯” 30-60% â†’ "å¯¹è¯ä¸å™è¿°å¹³è¡¡"
- å¯¹è¯å æ¯” < 30% â†’ "å™è¿°ä¸ºä¸»"
- å½¢å®¹è¯å¯†åº¦ < 5% â†’ "ç®€æ´å…‹åˆ¶"
- å½¢å®¹è¯å¯†åº¦ 5-10% â†’ "é€‚åº¦ä¿®é¥°"
- å½¢å®¹è¯å¯†åº¦ > 10% â†’ "æå†™ç»†è…»"

**ä¼ªä»£ç **ï¼š
```python
def generate_style_description(style_profile: Dict) -> str:
    """ç”Ÿæˆå®šæ€§é£æ ¼æè¿°"""
    descriptions = []

    # å¥å¼ç‰¹ç‚¹
    avg_length = style_profile['sentence']['avg_length']
    if avg_length < 18:
        descriptions.append("çŸ­å¥ä¸ºä¸»")
    elif avg_length < 25:
        descriptions.append("ä¸­çŸ­å¥æ··åˆ")
    else:
        descriptions.append("é•¿å¥ä¸ºä¸»")

    # å¯¹è¯ç‰¹ç‚¹
    dialogue_ratio = style_profile['dialogue']['dialogue_ratio']
    if dialogue_ratio > 60:
        descriptions.append("å¯¹è¯é©±åŠ¨")
    elif dialogue_ratio > 30:
        descriptions.append("å¯¹è¯ä¸å™è¿°å¹³è¡¡")
    else:
        descriptions.append("å™è¿°ä¸ºä¸»")

    # ç”¨è¯ç‰¹ç‚¹
    adj_density = style_profile['word']['adj_density']
    if adj_density < 5:
        descriptions.append("ç®€æ´å…‹åˆ¶")
    elif adj_density < 10:
        descriptions.append("é€‚åº¦ä¿®é¥°")
    else:
        descriptions.append("æå†™ç»†è…»")

    # èŠ‚å¥ç‰¹ç‚¹
    scenes = style_profile['scene']['avg_scenes_per_chapter']
    if scenes > 4:
        descriptions.append("å¿«èŠ‚å¥")
    elif scenes > 2:
        descriptions.append("ä¸­ç­‰èŠ‚å¥")
    else:
        descriptions.append("æ…¢èŠ‚å¥")

    return "ã€".join(descriptions)
```

---

## 8. ç”Ÿæˆ personal-voice.md

### 8.1 æ ¼å¼åŒ–è¾“å‡º

**ç›®æ ‡**ï¼šå°†åˆ†æç»“æœæ ¼å¼åŒ–ä¸º Markdown æ–‡ä»¶

**ä¼ªä»£ç **ï¼š
```python
def generate_personal_voice_md(style_profile: Dict, chapters_analyzed: int, total_words: int) -> str:
    """ç”Ÿæˆ personal-voice.md å†…å®¹"""
    from datetime import date

    today = date.today().strftime("%Y-%m-%d")
    style_desc = generate_style_description(style_profile)

    md_content = f"""# ä¸ªäººå†™ä½œé£æ ¼æŒ‡å—

> æœ¬æ–‡ä»¶ç”±é£æ ¼å­¦ä¹  Skill è‡ªåŠ¨ç”Ÿæˆï¼ŒåŸºäºå·²å†™ç« èŠ‚åˆ†æ
> ç”Ÿæˆæ—¥æœŸï¼š{today}
> åˆ†æç« èŠ‚ï¼šç¬¬ 1-{chapters_analyzed} ç« ï¼ˆå…± {total_words:,} å­—ï¼‰

---

## ğŸ“Š é‡åŒ–é£æ ¼ç‰¹å¾

### å¥å­å±‚é¢
- **å¹³å‡å¥é•¿**ï¼š{style_profile['sentence']['avg_length']} å­—
- **å¥é•¿åˆ†å¸ƒ**ï¼š
  - çŸ­å¥ï¼ˆ<15å­—ï¼‰ï¼š{style_profile['sentence']['short_ratio']}%
  - ä¸­å¥ï¼ˆ15-30å­—ï¼‰ï¼š{style_profile['sentence']['medium_ratio']}%
  - é•¿å¥ï¼ˆ>30å­—ï¼‰ï¼š{style_profile['sentence']['long_ratio']}%
- **å¥é•¿æ ‡å‡†å·®**ï¼š{style_profile['sentence']['std_dev']}ï¼ˆ{'ä½' if style_profile['sentence']['std_dev'] < 5 else 'ä¸­ç­‰' if style_profile['sentence']['std_dev'] < 10 else 'é«˜'}å˜åŒ–ï¼‰

### è¯æ±‡å±‚é¢
- **å½¢å®¹è¯å¯†åº¦**ï¼š{style_profile['word']['adj_density']}%ï¼ˆ{'ä½' if style_profile['word']['adj_density'] < 5 else 'ä¸­' if style_profile['word']['adj_density'] < 10 else 'é«˜'}ï¼‰
- **å‰¯è¯å¯†åº¦**ï¼š{style_profile['word']['adv_density']}%ï¼ˆ{'ä½' if style_profile['word']['adv_density'] < 5 else 'ä¸­' if style_profile['word']['adv_density'] < 10 else 'é«˜'}ï¼‰
- **å¯¹è¯æ ‡ç­¾é¢‘ç‡**ï¼š{style_profile['dialogue_tags']['tag_frequency']}%ï¼ˆ{'è¾ƒå°‘ä½¿ç”¨' if style_profile['dialogue_tags']['tag_frequency'] < 40 else 'é€‚åº¦ä½¿ç”¨' if style_profile['dialogue_tags']['tag_frequency'] < 70 else 'é¢‘ç¹ä½¿ç”¨'}"ä»–è¯´""å¥¹è¯´"ï¼‰
- **é«˜é¢‘è¯ Top 10**ï¼š{', '.join(f'{word}({count}æ¬¡)' for word, count in style_profile['frequent_words'][:10])}

### æ®µè½å±‚é¢
- **å¹³å‡æ®µé•¿**ï¼š{style_profile['paragraph']['avg_length']} å­—
- **å•å¥æ®µæ¯”ä¾‹**ï¼š{style_profile['paragraph']['single_para_ratio']}%ï¼ˆ{'ä½' if style_profile['paragraph']['single_para_ratio'] < 20 else 'ä¸­' if style_profile['paragraph']['single_para_ratio'] < 40 else 'é«˜'}ï¼Œ{'åˆ¶é€ èŠ‚å¥æ„Ÿ' if style_profile['paragraph']['single_para_ratio'] > 30 else ''}ï¼‰
- **æ®µè½ç±»å‹åˆ†å¸ƒ**ï¼š
  - å¯¹è¯æ®µï¼š{style_profile['paragraph_types']['dialogue_ratio']}%
  - å™è¿°æ®µï¼š{style_profile['paragraph_types']['narration_ratio']}%
  - æå†™æ®µï¼š{style_profile['paragraph_types']['description_ratio']}%

### å™è¿°å±‚é¢
- **å¯¹è¯å æ¯”**ï¼š{style_profile['dialogue']['dialogue_ratio']}%ï¼ˆ{'å¯¹è¯é©±åŠ¨' if style_profile['dialogue']['dialogue_ratio'] > 60 else 'å¹³è¡¡' if style_profile['dialogue']['dialogue_ratio'] > 30 else 'å™è¿°ä¸ºä¸»'}ï¼‰

---

## ğŸ¨ å®šæ€§é£æ ¼å°è±¡

### æ•´ä½“é£æ ¼
{style_desc}

[... å…¶ä½™å†…å®¹ä½¿ç”¨ SKILL.md ä¸­çš„æ¨¡æ¿ ...]

---

**ä½¿ç”¨å»ºè®®**ï¼š
æ¯æ¬¡æ‰§è¡Œ /write å‰ï¼Œå…ˆå¿«é€Ÿæ‰«ææœ¬æ–‡ä»¶ï¼Œåˆ·æ–°é£æ ¼è®°å¿†ã€‚
"""

    return md_content
```

---

## 9. å®ç°æ³¨æ„äº‹é¡¹

### 9.1 æ€§èƒ½ä¼˜åŒ–

- å¯¹äºé•¿æ–‡æœ¬ï¼ˆ> 10 ä¸‡å­—ï¼‰ï¼Œè€ƒè™‘åˆ†æ‰¹å¤„ç†
- ç¼“å­˜åˆ†è¯ç»“æœï¼Œé¿å…é‡å¤è®¡ç®—
- ä½¿ç”¨å¤šçº¿ç¨‹å¹¶è¡Œåˆ†æä¸åŒç« èŠ‚

### 9.2 é”™è¯¯å¤„ç†

- ç« èŠ‚æ•°ä¸è¶³ï¼ˆ< 3 ç« ï¼‰ï¼šæç¤ºç”¨æˆ·æ ·æœ¬ä¸è¶³
- æ–‡ä»¶è¯»å–å¤±è´¥ï¼šæä¾›æ¸…æ™°çš„é”™è¯¯ä¿¡æ¯
- åˆ†æç»“æœå¼‚å¸¸ï¼ˆå¦‚å¹³å‡å¥é•¿ < 5 æˆ– > 100ï¼‰ï¼šè­¦å‘Šç”¨æˆ·å¯èƒ½å­˜åœ¨é—®é¢˜

### 9.3 è¾¹ç•Œæƒ…å†µ

- ç©ºç« èŠ‚ï¼šè·³è¿‡
- çº¯å¯¹è¯ç« èŠ‚ï¼šè°ƒæ•´åˆ†ææƒé‡
- ç‰¹æ®Šæ ¼å¼ï¼ˆè¯—æ­Œã€ä¿¡ä»¶ï¼‰ï¼šè¯†åˆ«å¹¶ç‰¹æ®Šå¤„ç†

---

## 10. æµ‹è¯•éªŒè¯

### 10.1 å•å…ƒæµ‹è¯•

ä¸ºæ¯ä¸ªåˆ†æå‡½æ•°ç¼–å†™å•å…ƒæµ‹è¯•ï¼š
- `test_split_sentences()`: æµ‹è¯•å¥å­åˆ†å‰²
- `test_analyze_sentence_length()`: æµ‹è¯•å¥é•¿è®¡ç®—
- `test_tokenize_and_tag()`: æµ‹è¯•åˆ†è¯å’Œè¯æ€§æ ‡æ³¨
- `test_analyze_dialogue()`: æµ‹è¯•å¯¹è¯æ£€æµ‹

### 10.2 é›†æˆæµ‹è¯•

ä½¿ç”¨çœŸå®ç« èŠ‚è¿›è¡Œç«¯åˆ°ç«¯æµ‹è¯•ï¼š
- çŸ­å¥é£æ ¼æ ·æœ¬ï¼ˆå¹³å‡å¥é•¿ 12-15 å­—ï¼‰
- å¯¹è¯é©±åŠ¨æ ·æœ¬ï¼ˆå¯¹è¯å æ¯” > 70%ï¼‰
- æå†™ç»†è…»æ ·æœ¬ï¼ˆå½¢å®¹è¯å¯†åº¦ > 12%ï¼‰

### 10.3 éªŒè¯æ ‡å‡†

- åˆ†æç»“æœçš„å‡†ç¡®æ€§ï¼ˆä¸äººå·¥æ ‡æ³¨å¯¹æ¯”ï¼‰
- ç”Ÿæˆçš„é£æ ¼æŒ‡å—çš„å¯è¯»æ€§
- ç»­å†™æŒ‡å¯¼çš„å¯æ‰§è¡Œæ€§

---

## æ€»ç»“

æœ¬ç®—æ³•é€šè¿‡å¤šå±‚æ¬¡çš„æ–‡æœ¬åˆ†æï¼Œä»ç”¨æˆ·å·²å†™ç« èŠ‚ä¸­æå–å¯é‡åŒ–çš„é£æ ¼ç‰¹å¾ï¼Œå¹¶ç”Ÿæˆä¸ªæ€§åŒ–çš„é£æ ¼æŒ‡å—ã€‚æ ¸å¿ƒæ€è·¯æ˜¯ï¼š

1. **é‡åŒ–ä¸ºä¸»**ï¼šæå–å®¢è§‚ã€å¯æµ‹é‡çš„æŒ‡æ ‡
2. **å®šæ€§ä¸ºè¾…**ï¼šç”Ÿæˆæ˜“ç†è§£çš„é£æ ¼æè¿°
3. **æŒ‡å¯¼ä¸ºé‡**ï¼šæä¾›å…·ä½“çš„ç»­å†™å»ºè®®

é€šè¿‡è¿™ç§æ–¹å¼ï¼ŒAI å¯ä»¥çœŸæ­£ç†è§£ç”¨æˆ·çš„å†™ä½œé£æ ¼ï¼Œè€Œéä¾èµ–æ¨¡ç³Šçš„"å­¦æˆ‘çš„é£æ ¼"æŒ‡ä»¤ã€‚
